{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a424b44",
   "metadata": {},
   "source": [
    "# Speech Understanding \n",
    "# Lecture 10: Fourier analysis of natural speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3983e7c",
   "metadata": {},
   "source": [
    "### Mark Hasegawa-Johnson, KCGI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa13a99",
   "metadata": {},
   "source": [
    "* A synthetic vowel can be analyzed by taking the Fourier transform of the whole numpy array\n",
    "* Real speech changes over time\n",
    "* In order to cope with the changes over time, we can use the short-time Fourier transform (STFT)\n",
    "* The log magnitude STFT is called the spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ca0a6",
   "metadata": {},
   "source": [
    "Here are the contents:\n",
    "1. <a href=\"#section1\">Using ipywebrtc to record speech</a>\n",
    "1. <a href=\"#section2\">Using librosa to read speech audio files</a>\n",
    "1. <a href=\"#section3\">Calculating a spectrogram using `np.fft.fft`</a>\n",
    "1. <a href=\"#section4\">Calculating a spectrogram using librosa</a>\n",
    "1. <a href=\"#homework\">Homework</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5b87c",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1f1a1",
   "metadata": {},
   "source": [
    "## 1.  Using <a href=\"https://ipywebrtc.readthedocs.io/en/latest/\">ipywebrtc</a> to record speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958c9dc",
   "metadata": {},
   "source": [
    "[ipywebrtc](https://ipywebrtc.readthedocs.io/en/latest/) is a general package for recording audio and video into your Jupyter notebook.\n",
    "\n",
    "In order to use ipywebrtc, first you need to install it.  You can do that using the following shell command, or by issuing the same command without the \"!\" in a terminal window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e287d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywebrtc in d:\\anaconda\\lib\\site-packages (0.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (d:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (d:\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywebrtc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975c563",
   "metadata": {},
   "source": [
    "Recording audio using ipywebrtc is exactly the same as recording video, except that we create a `CameraStream` object with the constraints `'audio':True, 'video':False`.  With those constraints, the camera stream will record only audio, not video.\n",
    "\n",
    "When you the circle button is black, you can press it to start recording.  When it is red, you can press it to stop recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "005827bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ab0af269e94bde82e83305f1e6a890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AudioRecorder(audio=Audio(value=b'', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywebrtc\n",
    "camera = ipywebrtc.CameraStream(constraints={'audio': True,'video':False})\n",
    "recorder = ipywebrtc.AudioRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0382982",
   "metadata": {},
   "source": [
    "If we print the `recorder` object, we can see its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "487e353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioRecorder(audio=Audio(value=b'\\x1aE\\xdf\\xa3\\x9fB\\x86\\x81\\x01B\\xf7\\x81\\x01B\\xf2\\x81\\x04B\\xf3\\x81\\x08B\\x82\\x84webmB\\x87\\x81\\x04B\\x85...', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': False}))\n"
     ]
    }
   ],
   "source": [
    "print(recorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddab695",
   "metadata": {},
   "source": [
    "We can see that it is an `AudioRecorder` object, with the following member data:\n",
    "* audio - an audio object, containing\n",
    "   * value - the binary data of the recorded audio waveform\n",
    "   * format='webm' - the audio coding format in which audio.value is stored\n",
    "* stream - the `CameraStream` object from which the audio was recorded\n",
    "\n",
    "There are several ways to convert the `audio.value` data into a numpy array, but one of the simplest and most general is to just save it as a binary `.webm` file, then read it in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5835b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording.webm is a file containing 29603 bytes\n"
     ]
    }
   ],
   "source": [
    "with open('recording.webm','wb') as f:\n",
    "    f.write(recorder.audio.value)\n",
    "    \n",
    "import os\n",
    "print('recording.webm is a file containing',os.stat('recording.webm').st_size,'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a115a",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8148f",
   "metadata": {},
   "source": [
    "## 2. Using [librosa](https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html) to read speech audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c2dc3",
   "metadata": {},
   "source": [
    "[librosa](https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html) is a very general package for synthesizing and analyzing audio files.  For now, let's use it to read in the `.webm` file you just created.  First, you need to install it.  In this line, the `-q` flag prevents pip from printing anything unless there is an error.  If nothing is printed, then you have no errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe6fdb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in d:\\anaconda\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in d:\\anaconda\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in d:\\anaconda\\lib\\site-packages (from librosa) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in d:\\anaconda\\lib\\site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in d:\\anaconda\\lib\\site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: joblib>=0.14 in d:\\anaconda\\lib\\site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\anaconda\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in d:\\anaconda\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in d:\\anaconda\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in d:\\anaconda\\lib\\site-packages (from librosa) (1.4.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in d:\\anaconda\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in d:\\anaconda\\lib\\site-packages (from librosa) (4.7.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in d:\\anaconda\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in d:\\anaconda\\lib\\site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from lazy-loader>=0.1->librosa) (23.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in d:\\anaconda\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: appdirs in d:\\anaconda\\lib\\site-packages (from pooch>=1.1->librosa) (1.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\anaconda\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->pooch>=1.1->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->pooch>=1.1->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->pooch>=1.1->librosa) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->pooch>=1.1->librosa) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (d:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (d:\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c56b8",
   "metadata": {},
   "source": [
    "The [librosa.load](https://librosa.org/doc/latest/generated/librosa.load.html#librosa.load) function is able to read a wider variety of audio file formats than any other python package I know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f30ad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10449\\AppData\\Local\\Temp\\ipykernel_11980\\855123696.py:5: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  speech, Fs = librosa.load('recording.wav',sr=16000)\n"
     ]
    },
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'recording.wav': Format not recognised.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m speech, Fs \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecording.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m speech_time_axis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(speech))\u001b[38;5;241m/\u001b[39mFs\n\u001b[0;32m      8\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m3\u001b[39m),layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\librosa\\core\\audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\audioread\\__init__.py:132\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# All backends failed!\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NoBackendError()\n",
      "\u001b[1;31mNoBackendError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "speech, Fs = librosa.load('recording.wav',sr=16000)\n",
    "speech_time_axis = np.arange(len(speech))/Fs\n",
    "\n",
    "fig = plt.figure(figsize=(14,3),layout='tight')\n",
    "subplot = fig.subplots(1,1)\n",
    "subplot.plot(speech_time_axis,speech)\n",
    "subplot.set_xlabel('Time (seconds)',fontsize=18)\n",
    "subplot.set_title('Recording sampled at %d samples/second'%(Fs),fontsize=18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6953a",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea109e",
   "metadata": {},
   "source": [
    "## 3. Calculating a spectrogram using np.fft.fft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b91f8c",
   "metadata": {},
   "source": [
    "* A Fourier transform finds out which tones are present in a sound\n",
    "* ... but speech changes over time!\n",
    "\n",
    "In order to solve this problem, we use the **short-time Fourier transform**, or STFT.  The STFT has two steps:\n",
    "\n",
    "1. Divide `speech` into overlapping frames, resulting in a matrix\n",
    "1. Take the Fourier transform of each frame to get the **STFT**\n",
    "1. The **spectrogram** is the log magnitude of the STFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93215de8",
   "metadata": {},
   "source": [
    "### 3.1 Divide speech into overlapping frames\n",
    "\n",
    "Typically, we use frames that are about 0.025 seconds long (25 milliseconds), with a step of 0.01 seconds (10 milliseconds).\n",
    "\n",
    "There are an amazing number of different tricky ways to create overlapping frames in python; see [this page](https://stackoverflow.com/questions/2485669/consecutive-overlapping-subsets-of-array-numpy-python) for lots of methods.  The code below is intended to avoid tricky approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7885e134",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m frame_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.025\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mFs\u001b[49m)\n\u001b[0;32m      2\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.01\u001b[39m\u001b[38;5;241m*\u001b[39mFs)\n\u001b[0;32m      3\u001b[0m num_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((\u001b[38;5;28mlen\u001b[39m(speech)\u001b[38;5;241m-\u001b[39mframe_length)\u001b[38;5;241m/\u001b[39mstep)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Fs' is not defined"
     ]
    }
   ],
   "source": [
    "frame_length = int(0.025*Fs)\n",
    "step = int(0.01*Fs)\n",
    "num_frames = int((len(speech)-frame_length)/step)\n",
    "print('frame_length is',frame_length,'and step is',step,'and num_frames is',num_frames)\n",
    "\n",
    "speech_frames = np.zeros((frame_length, num_frames))\n",
    "for frame in range(num_frames):\n",
    "    speech_frames[:,frame] = speech[frame*step:frame*step+frame_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb940da8",
   "metadata": {},
   "source": [
    "We can plot this as an image.  Most of the samples will be near zero, and only a few will be strongly negative or strongly positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9cf5dcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m4\u001b[39m),layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m subplot \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m subplot\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mspeech_frames\u001b[49m,aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m subplot\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrame number\u001b[39m\u001b[38;5;124m'\u001b[39m,fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n\u001b[0;32m      5\u001b[0m subplot\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample number\u001b[39m\u001b[38;5;124m'\u001b[39m,fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speech_frames' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(14,4),layout='tight')\n",
    "subplot = fig.subplots(1,1)\n",
    "subplot.imshow(speech_frames,aspect='auto',origin='lower')\n",
    "subplot.set_xlabel('Frame number',fontsize=18)\n",
    "subplot.set_ylabel('Sample number',fontsize=18)\n",
    "subplot.set_title('Image of speech frames, 25ms frames with a 10ms step',fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976ef0e",
   "metadata": {},
   "source": [
    "### 3.2 Take the FFT of each frame to get the STFT\n",
    "\n",
    "The STFT is computed by taking the FFT of each frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c5183df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m speech_stft \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft(\u001b[43mspeech_frames\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m4\u001b[39m),layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m subplot \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speech_frames' is not defined"
     ]
    }
   ],
   "source": [
    "speech_stft = np.fft.fft(speech_frames, axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(14,4),layout='tight')\n",
    "subplot = fig.subplots(1,1)\n",
    "subplot.imshow(np.abs(speech_stft[:frame_length//2]),aspect='auto',origin='lower')\n",
    "subplot.set_xlabel('Frame number',fontsize=18)\n",
    "subplot.set_ylabel('Frequency bin',fontsize=18)\n",
    "subplot.set_title('Image of speech frames, 25ms frames with a 10ms step',fontsize=24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea6c34",
   "metadata": {},
   "source": [
    "### 3.3 The spectrogram is the log magnitude STFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd6ad8",
   "metadata": {},
   "source": [
    "As you can see from the plot above, the STFT is close to zero in most frequency bins.  Low-amplitude information can be made more visible by taking the logarithm.\n",
    "\n",
    "#### Standardization: Use decibels, `20*log10(abs(stft))`\n",
    "If you use `np.log` to calculate the logarithm, then your spectrogram units are not obvious.  If you use `20*np.log10`, then you can say that your spectrogram expresses levels in units of decibels.  The <a href=\"https://en.wikipedia.org/wiki/Decibel\">decibel</a> is an international standard way of expressing logarithms.  Since it's an international standard, it's nice to use it.\n",
    "\n",
    "#### Normalization: Normalize maximum to 0dB, Clip minimum to -60dB\n",
    "In order to avoid taking the logarithm of zero, it's a good idea to use `np.amax` and `np.maximum` so that the largest value is 0dB ($=20\\log_{10}(1)$), and the smallest value is -60dB ($=20\\log_{10}(0.001)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4118e3aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech_stft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mstft \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[43mspeech_stft\u001b[49m)\n\u001b[0;32m      2\u001b[0m speech_spectrogram \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog10(np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0.001\u001b[39m,mstft\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mamax(mstft)))\n\u001b[0;32m      4\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m4\u001b[39m),layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speech_stft' is not defined"
     ]
    }
   ],
   "source": [
    "mstft = np.abs(speech_stft)\n",
    "speech_spectrogram = 20*np.log10(np.maximum(0.001,mstft/np.amax(mstft)))\n",
    "                            \n",
    "fig = plt.figure(figsize=(14,4),layout='tight')\n",
    "subplot = fig.subplots(1,1)\n",
    "subplot.imshow(speech_spectrogram[:frame_length//2],aspect='auto',origin='lower')\n",
    "subplot.set_xlabel('Frame number',fontsize=18)\n",
    "subplot.set_ylabel('Frequency bin',fontsize=18)\n",
    "subplot.set_title('Image of speech frames, 25ms frames with a 10ms step',fontsize=24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564707ab",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606aa76c",
   "metadata": {},
   "source": [
    "## 4. Calculating a spectrogram using librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecdf67d",
   "metadata": {},
   "source": [
    "librosa contains some useful functions:\n",
    "\n",
    "* <a href=\"https://librosa.org/doc/main/generated/librosa.stft.html\">librosa.stft</a> will chop a signal into frames, and find the STFT\n",
    "* <a href=\"https://librosa.org/doc/main/generated/librosa.amplitude_to_db.html\">librosa.amplitude_to_db</a> converts the magnitude to decibels\n",
    "* <a href=\"https://librosa.org/doc/main/generated/librosa.display.specshow.html#librosa.display.specshow\">librosa.display.specshow</a> has some extra settings that are useful for making useful images of spectrograms; for example, it can label the X-axis in seconds, and the Y-axis in Hertz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "add5997c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.01\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mFs\u001b[49m)\n\u001b[0;32m      4\u001b[0m framelength \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.025\u001b[39m\u001b[38;5;241m*\u001b[39mFs)\n\u001b[0;32m      6\u001b[0m librosa_stft \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mstft(speech,hop_length\u001b[38;5;241m=\u001b[39mstep,win_length\u001b[38;5;241m=\u001b[39mframelength)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Fs' is not defined"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "step = int(0.01*Fs)\n",
    "framelength = int(0.025*Fs)\n",
    "\n",
    "librosa_stft = librosa.stft(speech,hop_length=step,win_length=framelength)\n",
    "librosa_spectrogram = librosa.amplitude_to_db(np.abs(librosa_stft))\n",
    "librosa.display.specshow(librosa_spectrogram,sr=Fs,hop_length=step,x_axis='s',y_axis='hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e813bc9",
   "metadata": {},
   "source": [
    "<a id=\"homework\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b32d4",
   "metadata": {},
   "source": [
    "## Homework "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9eeaa5",
   "metadata": {},
   "source": [
    "Homework will be graded on Github.com.  Edit the file in this directory called `homework10.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751d6e2",
   "metadata": {},
   "source": [
    "### Homework 10.1: waveform_to_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8c8138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function waveform_to_frames in module homework10:\n",
      "\n",
      "waveform_to_frames(waveform, frame_length, step)\n",
      "    Chop a waveform into overlapping frames.\n",
      "    \n",
      "    @params:\n",
      "    waveform (np.ndarray(N)) - the waveform\n",
      "    frame_length (scalar) - length of the frame, in samples\n",
      "    step (scalar) - step size, in samples\n",
      "    \n",
      "    @returns:\n",
      "    frames (np.ndarray((frame_length, num_frames))) - waveform chopped into frames\n",
      "    \n",
      "    num_frames should be at least int((len(speech)-frame_length)/step); it may be longer.\n",
      "    For every n and t such that 0 <= t*step+n <= N-1, it should be the case that \n",
      "       frames[n,t] = waveform[t*step+n]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib, homework10\n",
    "importlib.reload(homework10)\n",
    "help(homework10.waveform_to_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "779f2d5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(homework10)\n\u001b[1;32m----> 2\u001b[0m frames \u001b[38;5;241m=\u001b[39m homework10\u001b[38;5;241m.\u001b[39mwaveform_to_frames(\u001b[43mspeech\u001b[49m, frame_length, step)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(frames,origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m,aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speech' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(homework10)\n",
    "frames = homework10.waveform_to_frames(speech, frame_length, step)\n",
    "plt.imshow(frames,origin='lower',aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf95bf4",
   "metadata": {},
   "source": [
    "### Homework 10.2: frames_to_stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06fa71a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function frames_to_stft in module homework10:\n",
      "\n",
      "frames_to_stft(frames)\n",
      "    Take the FFT of every column of the frames matrix.\n",
      "    \n",
      "    @params:\n",
      "    frames (np.ndarray((frame_length, num_frames))) - the speech samples (real-valued)\n",
      "    \n",
      "    @returns:\n",
      "    stft (np.ndarray((frame_length,num_frames))) - the STFT (complex-valued)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(homework10)\n",
    "help(homework10.frames_to_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63efbf9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(homework10)\n\u001b[1;32m----> 2\u001b[0m stft \u001b[38;5;241m=\u001b[39m homework10\u001b[38;5;241m.\u001b[39mframes_to_stft(\u001b[43mframes\u001b[49m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mabs(stft[:frame_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m]),origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m,aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'frames' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(homework10)\n",
    "stft = homework10.frames_to_stft(frames)\n",
    "plt.imshow(np.abs(stft[:frame_length//2]),origin='lower',aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb726db",
   "metadata": {},
   "source": [
    "### Homework 10.3: stft_to_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77121659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stft_to_spectrogram in module homework10:\n",
      "\n",
      "stft_to_spectrogram(stft)\n",
      "    Calculate the level, in decibels, of each complex-valued sample of the STFT,\n",
      "    normalized so the highest value is 0dB, \n",
      "    and clipped so that the lowest value is -60dB.\n",
      "    \n",
      "    @params:\n",
      "    stft (np.ndarray((frame_length,num_frames))) - STFT (complex-valued)\n",
      "    \n",
      "    @returns:\n",
      "    spectrogram (np.ndarray((frame_length,num_frames)) - spectrogram (real-valued)\n",
      "    \n",
      "    The spectrogram should be expressed in decibels (20*log10(abs(stft)).\n",
      "    np.amax(spectrogram) should be 0dB.\n",
      "    np.amin(spectrogram) should be no smaller than -60dB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(homework10)\n",
    "help(homework10.stft_to_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86b2b544",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(homework10)\n\u001b[1;32m----> 2\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m homework10\u001b[38;5;241m.\u001b[39mstft_to_spectrogram(\u001b[43mstft\u001b[49m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(spectrogram[:frame_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m],origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m,aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stft' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(homework10)\n",
    "spectrogram = homework10.stft_to_spectrogram(stft)\n",
    "plt.imshow(spectrogram[:frame_length//2],origin='lower',aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2dbe8c",
   "metadata": {},
   "source": [
    "### Receiving your grade\n",
    "\n",
    "In order to receive a grade for your homework, you need to:\n",
    "\n",
    "1. Run the following code block on your machine.  The result may list some errors, and then in the very last line, it will show a score.  That score (between 0% and 100%) is the grade you have earned so far.  If you want to earn a higher grade, please continue editing `homework3.py`, and then run this code block again.\n",
    "1. When you are happy with your score (e.g., when it reaches 100%), choose `File` $\\Rightarrow$ `Save and Checkpoint`.  Then use `GitHub Desktop` to commit and push your changes.\n",
    "1. Make sure that the 100% shows on your github repo on github.com.  If it doesn't, you will not receive credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98a99327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EEE\n",
      "======================================================================\n",
      "ERROR: test_frames_to_stft (grade.Test)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\grade.py\", line 28, in test_frames_to_stft\n",
      "    frames = homework10.waveform_to_frames(speech, frame_length, step)\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\homework10.py\", line 20, in waveform_to_frames\n",
      "    num_frames = int(len(speech)-frame_length/step)\n",
      "NameError: name 'speech' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_stft_to_spectrogram (grade.Test)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\grade.py\", line 46, in test_stft_to_spectrogram\n",
      "    frames = homework10.waveform_to_frames(speech, frame_length, step)\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\homework10.py\", line 20, in waveform_to_frames\n",
      "    num_frames = int(len(speech)-frame_length/step)\n",
      "NameError: name 'speech' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_waveform_to_frames (grade.Test)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\grade.py\", line 13, in test_waveform_to_frames\n",
      "    frames = homework10.waveform_to_frames(speech, frame_length, step)\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\homework10.py\", line 20, in waveform_to_frames\n",
      "    num_frames = int(len(speech)-frame_length/step)\n",
      "NameError: name 'speech' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.010s\n",
      "\n",
      "FAILED (errors=3)\n",
      "EEE\n",
      "======================================================================\n",
      "ERROR: test_frames_to_stft (grade.Test)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\grade.py\", line 28, in test_frames_to_stft\n",
      "    frames = homework10.waveform_to_frames(speech, frame_length, step)\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\homework10.py\", line 20, in waveform_to_frames\n",
      "    num_frames = int(len(speech)-frame_length/step)\n",
      "NameError: name 'speech' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_stft_to_spectrogram (grade.Test)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\grade.py\", line 46, in test_stft_to_spectrogram\n",
      "    frames = homework10.waveform_to_frames(speech, frame_length, step)\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\homework10.py\", line 20, in waveform_to_frames\n",
      "    num_frames = int(len(speech)-frame_length/step)\n",
      "NameError: name 'speech' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_waveform_to_frames (grade.Test)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\grade.py\", line 13, in test_waveform_to_frames\n",
      "    frames = homework10.waveform_to_frames(speech, frame_length, step)\n",
      "  File \"C:\\Users\\10449\\Documents\\GitHub\\intro_speech_understanding\\2024_spring\\lec10\\homework10.py\", line 20, in waveform_to_frames\n",
      "    num_frames = int(len(speech)-frame_length/step)\n",
      "NameError: name 'speech' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.006s\n",
      "\n",
      "FAILED (errors=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 successes out of 3 tests run\n",
      "Score: 0%\n",
      "0 successes out of 3 tests run\n",
      "Score: 0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'grade' from 'C:\\\\Users\\\\10449\\\\Documents\\\\GitHub\\\\intro_speech_understanding\\\\2024_spring\\\\lec10\\\\grade.py'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, grade\n",
    "importlib.reload(grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5fdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
